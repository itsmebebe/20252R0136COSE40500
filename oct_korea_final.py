# -*- coding: utf-8 -*-
"""OCT_korea_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_RHMzEgKJttAk44SjnIVT1V0XWaQvYUh
"""

!pip install kaggle

import os
import zipfile
import random
import shutil
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from glob import glob

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
import kagglehub
# -------------------------------
# Configuration Flags
# -------------------------------
USE_GEOMETRIC_AUG = True  # Set to False to disable geometric augmentations
USE_GAN_AUG = False       # Set to True to enable GAN-based augmentation

path = kagglehub.dataset_download("paultimothymooney/kermany2018")
print("Path to dataset files:", path)

# The dataset file is typically `OCT2017.tar.gz` inside the downloaded folder.
# Let's search for the file and extract it.

try:
    contents = os.listdir(path)
    print("Contents of the directory:")
    for item in contents:
        print(item)
except FileNotFoundError:
    print(f"Path '{path}' does not exist.")
except PermissionError:
    print(f"Permission denied to access '{path}'.")

def find_oct2017_dir(base_path):
    for root, dirs, files in os.walk(base_path):
        print(dirs)
        if 'OCT2017 ' in dirs:
            print("yes")
            return os.path.join(root, 'OCT2017 ')
    return None

oct2017_dir = find_oct2017_dir(path)
if oct2017_dir is None:
    raise FileNotFoundError("Could not find OCT2017 directory in the downloaded dataset.")
print("OCT2017 directory found at:", oct2017_dir)

import os
import random
from glob import glob
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F
import random
# Replace with your dataset directory
oct2017_dir = oct2017_dir
USE_GEOMETRIC_AUG = True  # Set this as needed
USE_GAN_AUG = False       # Set this as needed

train_dir = os.path.join(oct2017_dir, "train")
test_dir = os.path.join(oct2017_dir, "test")

if not os.path.isdir(train_dir):
    raise FileNotFoundError(f"Train directory not found at {train_dir}")
if not os.path.isdir(test_dir):
    raise FileNotFoundError(f"Test directory not found at {test_dir}")

classes = os.listdir(train_dir)
print("Classes found:", classes)

# -------------------------------
# Data Augmentation Setup
# -------------------------------
class RandomOcclusion:
    def __init__(self, max_size=30):
        self.max_size = max_size

    def __call__(self, img):
        width, height = img.size
        x = random.randint(0, width - self.max_size)
        y = random.randint(0, height - self.max_size)
        overlay = Image.new('L', (self.max_size, self.max_size), color=0)  # Black square
        img.paste(overlay, (x, y))
        return img

occlusion_transform = RandomOcclusion(max_size=30)

jittering_transform = transforms.ColorJitter(brightness=0.3, contrast=0.3)

blurring_transform = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))

def random_transform(img):
    """
    Apply one of the transformations (Occlusion, Jittering, Blurring, Normal)
    with 25% probability each.
    """
    choice = random.random()  # Random value between 0 and 1
    if choice < 0.25:
        return occlusion_transform(img)  # 25% chance for Occlusion
    elif choice < 0.5:
        return jittering_transform(img)  # 25% chance for Jittering
    elif choice < 0.75:
        return blurring_transform(img)  # 25% chance for Blurring
    else:
        return img  # 25% chance for Normal (no transformation)

train1_transforms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
# Apply random_transform in the data augmentation pipeline
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Lambda(lambda img: random_transform(img)),  # Apply one of the four transformations
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3)
])


test_transforms = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.Lambda(lambda img: random_transform(img)),  # Apply one of the four transformations
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# -------------------------------
# Dataset and Dataloaders
# -------------------------------
train_dataset = datasets.ImageFolder(train_dir, transform=train1_transforms)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)

print("Number of training images:", len(train_dataset))
print("Number of test images:", len(test_dataset))



# 원하는 샘플 개수
undersample_counts = {
    "DME":10000,
    "DRUSEN":10000,
    "NORMAL": 12000,  # NORMAL 클래스에서 15,000개 샘플
    "CNV": 15000      # CNV 클래스에서 18,000개 샘플
}


from torchvision.datasets import ImageFolder

# 전체 데이터셋 로드 (ImageFolder)
full_dataset = ImageFolder(train_dir, transform=train_transforms)

# 클래스별로 이미지 파일 인덱스를 저장
class_to_indices = {cls: [] for cls in classes}

for idx, (_, label) in enumerate(full_dataset.imgs):
    class_name = classes[label]
    class_to_indices[class_name].append(idx)

# Undersampling 수행
selected_indices = []
for class_name, max_count in undersample_counts.items():
    indices = class_to_indices[class_name]
    if len(indices) > max_count:
        selected_indices.extend(random.sample(indices, max_count))  # 무작위 샘플링
    else:
        selected_indices.extend(indices)  # 샘플 수가 이미 작으면 그대로 사용

# Subset 생성
undersampled_dataset = torch.utils.data.Subset(full_dataset, selected_indices)

# 새로운 DataLoader 생성
train_loader = DataLoader(undersampled_dataset, batch_size=32, shuffle=True, num_workers=2)

print(f"Undersampled Dataset Size: {len(undersampled_dataset)}")

train1_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

# 클래스별 샘플 개수 확인
class_counts = {cls: 0 for cls in classes}  # 클래스별 카운트를 저장할 딕셔너리 초기화

# 선택된 인덱스에 대해 클래스별 카운트 누적
for idx in selected_indices:
    class_name = classes[full_dataset.targets[idx]]  # 해당 인덱스의 클래스 이름
    class_counts[class_name] += 1  # 카운트 증가

# 클래스별 개수 출력
print("Class counts after undersampling:")
for class_name, count in class_counts.items():
    print(f"{class_name}: {count}")

# -------------------------------
# Define a Custom Model
# -------------------------------
class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        # A simple CNN architecture:
        # Input: (3 x 224 x 224)
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)

        # Compute the size of the feature map after the convolutional layers
        # After three conv + pool layers (assuming pooling after each conv):
        # 224 -> pool -> 112 -> pool -> 56 -> pool -> 28 (if we pool after each conv block)
        # Let's apply pooling after each of the first two conv sets for simplicity.
        # Actually, let's do pooling after each convolution layer for demonstration:
        # conv1 -> pool: (32 filters) output shape: (32, 112, 112)
        # conv2 -> pool: (64 filters) output shape: (64, 56, 56)
        # conv3 -> pool: (128 filters) output shape: (128, 28, 28)
        # Flatten size = 128 * 28 * 28

        # We'll flatten and then use two fully connected layers
        self.fc1 = nn.Linear(128 * 28 * 28, 256)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        # Conv + ReLU + Pool block 1
        x = self.pool(F.relu(self.conv1(x)))  # (32, 112, 112)

        # Conv + ReLU + Pool block 2
        x = self.pool(F.relu(self.conv2(x)))  # (64, 56, 56)

        # Conv + ReLU + Pool block 3
        x = self.pool(F.relu(self.conv3(x)))  # (128, 28, 28)

        # Flatten
        x = x.view(x.size(0), -1)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = SimpleCNN(num_classes=len(classes))
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# -------------------------------
# Training and Evaluation Functions
# -------------------------------
def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    total = 0
    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        _, preds = torch.max(outputs, 1)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
        total += inputs.size(0)
    epoch_loss = running_loss / total
    epoch_acc = running_corrects.double().item() / total
    return epoch_loss, epoch_acc

def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
            total += inputs.size(0)
    epoch_loss = running_loss / total
    epoch_acc = running_corrects.double().item() / total
    return epoch_loss, epoch_acc

# -------------------------------
# Training Loop
# -------------------------------
num_epochs = 4
for epoch in range(num_epochs):
    train_loss, train_acc = train_one_epoch(model, train1_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, test_loader, criterion, device)
    print(f"Epoch {epoch+1}/{num_epochs}: "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

num_epochs = 2
for epoch in range(num_epochs):
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, test_loader, criterion, device)
    print(f"Epoch {epoch+1}/{num_epochs}: "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

import zipfile
import os
import pandas as pd
from torchvision import transforms
from PIL import Image
import torch

# Define the label mapping
label_map = {
    "CNV": 0,
    "DME": 1,
    "DRUSEN": 2,
    "NORMAL": 3
}

# Path to the zip file
zip_file_path = "augmented_test.zip"  # Replace with the correct path to your zip file
extracted_folder = "augmented_test"  # Folder to extract the images

# Extract the zip file
if not os.path.exists(extracted_folder):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extracted_folder)
print(f"Extracted test images to: {extracted_folder}")

# Load the trained model
model.eval()

# Create a list to store results
results = []

# Define the test transformations (same as used during training)
test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5] * 3, [0.5] * 3)
])

# Process each image in the extracted folder
for image_name in sorted(os.listdir(extracted_folder)):  # Sort to ensure numerical order
    image_path = os.path.join(extracted_folder, image_name)

    if image_name.endswith('.jpeg') or image_name.endswith('.jpg') or image_name.endswith('.png'):
        # Open and preprocess the image
        image = Image.open(image_path).convert("L").convert("RGB")
        image = test_transforms(image)
        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device

        # Predict the label
        with torch.no_grad():
            output = model(image)
            _, predicted = torch.max(output, 1)  # Get the index of the highest score

        # Get the class name from the index
        class_name = classes[predicted.item()]
        label_number = label_map[class_name]

        # Append to results
        results.append({"Image Name": image_name, "Predicted Label": class_name, "Label Number": label_number})

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Save to Excel
output_file = "augmented_test_predictions.xlsx"
results_df.to_excel(output_file, index=False)

print(f"Predictions saved to {output_file}")

#train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
val_loss, val_acc = evaluate(model, test_loader, criterion, device)
#print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")